hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  gen_batch_size: ${data.train_batch_size}
  return_raw_chat: False
  filter_prompts: True
  max_target_length: 8192
  filter_targets: False
  sample_target_ratio: 1.0

reward_model:
  reward_manager: dapo
  model:
    input_tokenizer: null
  overlong_buffer: 
    enable: False # We try to avoid forgetting to set enable
    len: 0
    penalty_factor: 0.0
    log: False

answers_checker:
  ulysses_sequence_parallel_size: 1
  use_remove_padding: True
  model:
    fsdp_config:
      wrap_policy:
        min_num_params: 0
      param_offload: False
      reshard_after_forward: True
      fsdp_size: -1
    forward_prefetch: False
  # 配置vllm的参数
  free_cache_engine: False
  tensor_parallel_size: 2
  dtype: bfloat16
  enforce_eager: True
  gpu_memory_utilization: 0.5
  load_format: dummy_dtensor
  disable_log_stats: True
  max_num_batched_tokens: 8192
  max_model_len: 8192
  enable_chunked_prefill: False
  embedding_model:
    enable: True
    path: Qwen3/Qwen3-Embedding-4B
  similarity_threshold: 0.2
  max_blocks_num: 10

algorithm:
  filter_groups:
    _target_: verl.trainer.config.FilterGroupsConfig
    enable: False # We try to avoid forgetting to set enable
    metric: null # acc / score / seq_reward / seq_final_reward / ...
    max_num_gen_batches: 0 # Non-positive values mean no upper limit

actor_rollout_ref:
  rollout:
    calculate_log_probs: False
    n_off_policy: 1
    self_explain:
      max_tokens: 100
      max_blocks_num: 5
    chunk_size: 8
  actor:
    use_off_policy_loss: False
    off_policy_normalize: False
    off_policy_loss_impl: token
    off_policy_cliprange: 0.2
    use_off_policy_probs: False
    use_off_policy_clip: False
    off_policy_max_clip: -1
    off_policy_min_clip: -1
    off_policy_reshape: "no_reshape"
    off_policy_reshape_weight: 0.1
    off_policy_reshape_pow_exp: 0.5
    on_policy_reshape: "no_reshape"
    on_policy_reshape_weight: 0.1
    on_policy_reshape_pow_exp: 0.5
    calculate_sft_loss: True
    all_max_clip: -1
    loss_remove_token_mean: False
    loss_remove_clip: False
    sft_loss_coef: 0.1 # 仅在calculate_sft_loss为True时生效
    calculate_rl_loss: True

trainer:
  project_name: verl-dapo
  need_analyze_sft_grads: False
  need_analyze_off_grads: False
  analyze_gradients_freq: 10
